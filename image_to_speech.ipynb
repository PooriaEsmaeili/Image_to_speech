{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "HUGGINGACEHUB_API_TOKEN = os.getenv('HUGGINGACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img2text\n",
    "def img2text(url):\n",
    "    image_to_text = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "    text = image_to_text(url)[0]['generated_text']\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate story\n",
    "def generate_short_story(context):\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "   \n",
    "    input_text = f\"Generate a 20 words short story based on the following context: {context}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    outputs = model.generate(**input_ids, max_length=20, num_return_sequences=1, early_stopping=True)\n",
    "\n",
    "    generated_story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_story\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate story\n",
    "def generate_short_story2(context):\n",
    "\n",
    "    pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "   \n",
    "    input_text = f\"Generate a short strory based on this context: {context}\"\n",
    "\n",
    "    outputs = pipe(input_text, max_length=20, num_return_sequences=1)\n",
    "\n",
    "    generated_story = outputs[0][\"generated_text\"]\n",
    "    return generated_story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2speech(message):\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/espnet/kan-bayashi_ljspeech_vits\"\n",
    "    headers = {\"Authorization\": f\"Bearer {HUGGINGACEHUB_API_TOKEN }\"}\n",
    "    payloads = {\n",
    "        \"inputs\": message\n",
    "    }\n",
    "    response = requests.post(API_URL, headers=headers, json=payloads)\n",
    "    with open('audio.flac', 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family fun at the beach\n"
     ]
    }
   ],
   "source": [
    "sc = img2text(\"people.jpeg\")\n",
    "story = generate_short_story2(sc)\n",
    "text2speech(story)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2251c95a5384293cf96eec725e40059bf9dbc546a4a300f41156f5c39f262e2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
